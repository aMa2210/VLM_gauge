{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "RESULTS_DIR = 'Results/2.- Analog Depth Gaugegemini-3-pro-preview_naive'\n",
    "GT_DIR = 'Dataset/2_label'\n",
    "SUMMARY_OUTPUT = 'Analysed/Analysis_Summary_Depth_pro_naive.xlsx'\n",
    "\n",
    "def calculate_metrics(vlm_df, gt_df):\n",
    "    \"\"\"Calculate MAE, RMSE and TMS \"\"\"\n",
    "    # removing whitespace and unifying letter case.\n",
    "    gt_df.columns = [str(c).strip().lower() for c in gt_df.columns]\n",
    "\n",
    "    vlm_df.columns = [str(c).strip().lower() for c in vlm_df.columns]\n",
    "    \n",
    "    gt_df = gt_df.sort_values('timestamp_ms')\n",
    "    vlm_df = vlm_df.sort_values('ts_ms')\n",
    "    \n",
    "    # Nearest Neighbor\n",
    "    merged = pd.merge_asof(\n",
    "        vlm_df,\n",
    "        gt_df,\n",
    "        left_on='ts_ms',\n",
    "        right_on='timestamp_ms',\n",
    "        direction='nearest',\n",
    "        tolerance=200  # maximam 200ms error\n",
    "    ).dropna(subset=['encoder_verified_reading', 'reading'])\n",
    "\n",
    "    if len(merged) < 2:\n",
    "        print('error code 9137')\n",
    "        return None\n",
    "\n",
    "    rvlm = merged['reading'].values\n",
    "    rgt = merged['encoder_verified_reading'].values\n",
    "\n",
    "    mae = np.mean(np.abs(rvlm - rgt))\n",
    "    rmse = np.sqrt(np.mean((rvlm - rgt)**2))\n",
    "    \n",
    "    diff_vlm = np.sign(np.diff(rvlm))\n",
    "    diff_gt = np.sign(np.diff(rgt))\n",
    "    tms = np.mean(diff_vlm == diff_gt)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"TMS\": tms,\n",
    "        \"Valid_Samples\": len(merged)\n",
    "    }\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    try:\n",
    "        clean_text = re.sub(r'```json|```', '', text).strip()\n",
    "        return json.loads(clean_text)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        return None\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for file_name in os.listdir(RESULTS_DIR):\n",
    "    if not file_name.endswith('_Raw_Results.xlsx'):\n",
    "        continue\n",
    "    \n",
    "    seq_id = file_name.replace('_Raw_Results.xlsx', '')\n",
    "    print(f\"Analysing: {seq_id}...\")\n",
    "\n",
    "    vlm_raw_path = os.path.join(RESULTS_DIR, file_name)\n",
    "    vlm_raw_df = pd.read_excel(vlm_raw_path)\n",
    "    \n",
    "    raw_response = vlm_raw_df.iloc[0]['raw_model_response']\n",
    "    vlm_list = extract_json_from_text(raw_response)\n",
    "    \n",
    "    if not vlm_list:\n",
    "        print('error code 3315')\n",
    "        continue\n",
    "    vlm_df = pd.DataFrame(vlm_list)\n",
    "\n",
    "    gt_path = os.path.join(GT_DIR, f\"{seq_id}.xlsx\")\n",
    "    \n",
    "    if not os.path.exists(gt_path):\n",
    "        print(f\"{gt_path} does not exist\")\n",
    "        print('error code 88632')\n",
    "        continue\n",
    "        \n",
    "#     gt_df = pd.read_excel(gt_path)\n",
    "    gt_df = pd.read_excel(gt_path, sheet_name='Ground_Truth')\n",
    "    \n",
    "    metrics = calculate_metrics(vlm_df, gt_df)\n",
    "    \n",
    "    if metrics:\n",
    "        metrics['sequence_id'] = seq_id\n",
    "        summary_data.append(metrics)\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "metrics_cols = ['MAE', 'RMSE', 'TMS']\n",
    "stats_mean = summary_df[metrics_cols].mean()\n",
    "stats_std = summary_df[metrics_cols].std()\n",
    "\n",
    "table_2_rows = []\n",
    "for m in metrics_cols:\n",
    "    table_2_rows.append({\n",
    "        \"Metric\": m,\n",
    "        \"Mean ± SD\": f\"{stats_mean[m]:.4f} ± {stats_std[m]:.4f}\"\n",
    "    })\n",
    "table_2_df = pd.DataFrame(table_2_rows)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(SUMMARY_OUTPUT) as writer:\n",
    "    table_2_df.to_excel(writer, sheet_name='Summary_Statistics', index=False)\n",
    "    \n",
    "    cols_order = ['sequence_id', 'Valid_Samples', 'MAE', 'RMSE', 'TMS']\n",
    "    summary_df[cols_order].to_excel(writer, sheet_name='Detailed_Metrics', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"Results saved to: {SUMMARY_OUTPUT}\")\n",
    "print(\"\\n--- Overall Performance ---\")\n",
    "print(table_2_df.to_string(index=False))\n",
    "print(\"\\n--- Preview of Detailed Data ---\")\n",
    "print(summary_df[cols_order].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041764b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "TASK_CONFIGS = [\n",
    "    {\n",
    "        \"results_dir\": 'Results/2.- Analog Depth Gaugegemini-3-flash-preview_naive',\n",
    "        \"gt_dir\": 'Dataset/2_label',\n",
    "        \"label\": \"depth\"\n",
    "    },\n",
    "    {\n",
    "        \"results_dir\": 'Results/1.- Analog Dial Gaugegemini-3-flash-preview_naive',\n",
    "        \"gt_dir\": 'Dataset/1_label',\n",
    "        \"label\": \"dial\"\n",
    "    }\n",
    "]\n",
    "\n",
    "SUMMARY_OUTPUT = 'Analysed/Combined_Analysis_Summary_flash_naive.xlsx'\n",
    "\n",
    "\n",
    "def calculate_metrics(vlm_df, gt_df):\n",
    "\n",
    "    gt_df.columns = [str(c).strip().lower() for c in gt_df.columns]\n",
    "    vlm_df.columns = [str(c).strip().lower() for c in vlm_df.columns]\n",
    "    \n",
    "    gt_df = gt_df.sort_values('timestamp_ms')\n",
    "    vlm_df = vlm_df.sort_values('ts_ms')\n",
    "    \n",
    "    merged = pd.merge_asof(\n",
    "        vlm_df,\n",
    "        gt_df,\n",
    "        left_on='ts_ms',\n",
    "        right_on='timestamp_ms',\n",
    "        direction='nearest',\n",
    "        tolerance=200\n",
    "    ).dropna(subset=['encoder_verified_reading', 'reading'])\n",
    "\n",
    "    if len(merged) < 2:\n",
    "        return None\n",
    "\n",
    "    rvlm = merged['reading'].values\n",
    "    rgt = merged['encoder_verified_reading'].values\n",
    "\n",
    "    mae = np.mean(np.abs(rvlm - rgt))\n",
    "    rmse = np.sqrt(np.mean((rvlm - rgt)**2))\n",
    "    \n",
    "    diff_vlm = np.sign(np.diff(rvlm))\n",
    "    diff_gt = np.sign(np.diff(rgt))\n",
    "    tms = np.mean(diff_vlm == diff_gt)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"TMS\": tms,\n",
    "        \"Valid_Samples\": len(merged)\n",
    "    }\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    try:\n",
    "        clean_text = re.sub(r'```json|```', '', text).strip()\n",
    "        return json.loads(clean_text)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        return None\n",
    "    \n",
    "summary_data = []\n",
    "\n",
    "for config in TASK_CONFIGS:\n",
    "    res_dir = config[\"results_dir\"]\n",
    "    gt_dir = config[\"gt_dir\"]\n",
    "    tag = config[\"label\"]\n",
    "    \n",
    "    print(f\"\\n>>> Processing: {tag}\")\n",
    "    \n",
    "    if not os.path.exists(res_dir):\n",
    "        print(f\"result file does not exist: {res_dir}\")\n",
    "        continue\n",
    "\n",
    "    for file_name in os.listdir(res_dir):\n",
    "        if not file_name.endswith('_Raw_Results.xlsx'):\n",
    "            continue\n",
    "        \n",
    "        seq_id = file_name.replace('_Raw_Results.xlsx', '')\n",
    "        vlm_raw_path = os.path.join(res_dir, file_name)\n",
    "        \n",
    "        try:\n",
    "\n",
    "            vlm_raw_df = pd.read_excel(vlm_raw_path)\n",
    "            raw_response = vlm_raw_df.iloc[0]['raw_model_response']\n",
    "            vlm_list = extract_json_from_text(raw_response)\n",
    "            \n",
    "            if not vlm_list: continue\n",
    "            vlm_df = pd.DataFrame(vlm_list)\n",
    "\n",
    "\n",
    "            gt_path = os.path.join(gt_dir, f\"{seq_id}.xlsx\")\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"GT file {gt_path} does not exist\")\n",
    "                continue\n",
    "                \n",
    "            gt_df = pd.read_excel(gt_path, sheet_name='Ground_Truth')\n",
    "            \n",
    "\n",
    "            metrics = calculate_metrics(vlm_df, gt_df)\n",
    "            \n",
    "            if metrics:\n",
    "                metrics['sequence_id'] = seq_id\n",
    "                metrics['group_tag'] = tag \n",
    "                summary_data.append(metrics)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error when processing {file_name}: {e}\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "metrics_cols = ['MAE', 'RMSE', 'TMS']\n",
    "stats_mean = summary_df[metrics_cols].mean()\n",
    "stats_std = summary_df[metrics_cols].std()\n",
    "\n",
    "table_overall = pd.DataFrame([{\n",
    "    \"Metric\": m,\n",
    "    \"Combined Mean ± SD\": f\"{stats_mean[m]:.4f} ± {stats_std[m]:.4f}\"\n",
    "} for m in metrics_cols])\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(SUMMARY_OUTPUT) as writer:\n",
    "    table_overall.to_excel(writer, sheet_name='Overall_Statistics', index=False)\n",
    "\n",
    "    cols_order = ['group_tag', 'sequence_id', 'Valid_Samples', 'MAE', 'RMSE', 'TMS']\n",
    "    summary_df[cols_order].to_excel(writer, sheet_name='Detailed_Metrics', index=False)\n",
    "\n",
    "print(f\"\\nsaved to: {SUMMARY_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc263a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb56b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM_PHD)",
   "language": "python",
   "name": "llm_phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
